{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtU4mDEYZ9M8"
   },
   "source": [
    "Welcome back!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M03AZpV0Z_MB"
   },
   "source": [
    "In this lab, we will observe how weights and bias are trained over epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPbvP3m_aEOD"
   },
   "source": [
    "Let's begin with importing the libraries and read the dataset. We will also break the dataset into features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "#IGNORE ANY WARNINGS THAT MAY APPEAR ON THIS CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVBmL3VQupJK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIfPjSbyuzrq"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IioRKfbIu6PI"
   },
   "outputs": [],
   "source": [
    "x = data[['radius_mean', 'perimeter_mean']]\n",
    "y = data[['diagnosis']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96bmndFOaoVI"
   },
   "source": [
    "We are using the same dataset as in the last lab- view the inputs and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ZXziEFc1vGNq",
    "outputId": "b9202804-9dc9-4ba0-bec0-60948172ad2c"
   },
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "eibY5ziUvIf8",
    "outputId": "607162f3-da8f-4a86-9554-1fd25a2461a1"
   },
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYuaxNUjatXP"
   },
   "source": [
    "Using the same process as the last lab, we will normalize the inputs, and encode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCSM1S91vKNL"
   },
   "outputs": [],
   "source": [
    "#Normalize Input\n",
    "stats = x.describe().T\n",
    "mu, sigma = stats['mean'], stats['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBqwMMgnvbHg"
   },
   "outputs": [],
   "source": [
    "x_norm = (x-mu)/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "cM2yX4I0voPL",
    "outputId": "8445efdf-bf5c-462b-9085-75e23444b4c5"
   },
   "outputs": [],
   "source": [
    "x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1GxT_Dpvp9J"
   },
   "outputs": [],
   "source": [
    "rule = lambda val: 1 if val=='M' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sO2mtujSvxQj"
   },
   "outputs": [],
   "source": [
    "y=y['diagnosis'].apply(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "akEs2VtJwHLE",
    "outputId": "57d58e0b-68ad-45e1-ab2b-5353d7c1cea6"
   },
   "outputs": [],
   "source": [
    "y.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5bh3aV2a4UE"
   },
   "source": [
    "Now we will proceed to splitting the data. No changes so far-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tnpd4_9333a"
   },
   "outputs": [],
   "source": [
    "HP_epochs = 200\n",
    "HP_batch_size = 16\n",
    "HP_lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cA5q4R7awUHh"
   },
   "outputs": [],
   "source": [
    "xtrain,xtest,ytrain,ytest = train_test_split(x_norm,y,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGeLDgRdbz1I"
   },
   "source": [
    "Our dataset is small. But for big data, or larger datasets, we can use TensorFlow's Dataset API. \n",
    "<br/>\n",
    "We additionally shuffle the dataset to provide randomness. \n",
    "\n",
    "Because we are going to simulate the training process over epochs, we will also repeat our dataset. \n",
    "\n",
    "We will also divide our data into fixed size batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MkdzOqkS3vY4"
   },
   "outputs": [],
   "source": [
    "celldata = tf.data.Dataset.from_tensor_slices((xtrain,ytrain))\n",
    "celldata = celldata.shuffle(100).repeat(HP_epochs).batch(HP_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGS85CvycTZ6"
   },
   "source": [
    "This is the simplest way to create a dataset. \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(any_tensor_or_numpy_array_or_pandas_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tadQfoZvjsEf"
   },
   "source": [
    "Now we will place an iterator on top of our dataset to loop into the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9IE79euu3zb6"
   },
   "outputs": [],
   "source": [
    "itr = celldata.__iter__()\n",
    "#IGNORE ANY WARNINGS THAT MAY APPEAR ON THIS CELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdoyWzlrj2AY"
   },
   "source": [
    "Now, we will initialize our weights and bias. We are initializing the weights with random numbers, normally distributed from the mean 0, with a standard deviation of 0.5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXAFD2iv4BFR"
   },
   "outputs": [],
   "source": [
    "# we have only 2 features and 1 label\n",
    "# One weight for each feature\n",
    "w = tf.random.normal( ( 2 , 1 ), mean = 0, stddev =0.5,dtype=tf.dtypes.double ) \n",
    "# One bias for every output \n",
    "# our only output is the label prediction on 1 layer\n",
    "b = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lU4StrXkTKN"
   },
   "source": [
    "Have a look at the initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WVEzzuK-4MWb",
    "outputId": "ec86a455-cf73-477c-e3ba-800f06c468c2"
   },
   "outputs": [],
   "source": [
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2DSoKOdE4M_k"
   },
   "outputs": [],
   "source": [
    "# define an empty array to store all the errors\n",
    "# errors will be generated from each training simulation\n",
    "err = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMhkhf4ekhns"
   },
   "source": [
    "Let's define a dummy classifier to encode our predictions into 1 and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_MPWwSP4Qu3"
   },
   "outputs": [],
   "source": [
    "\n",
    "def myclassifier(vals):\n",
    "  res = []\n",
    "  rule = lambda r: 1.0 if r>=0.5 else 0.0\n",
    "  for val in vals:\n",
    "    res.append(rule(val))\n",
    "  return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h06Kz66_ktDV"
   },
   "source": [
    "And let's begin the training simulation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBje5KSOlVJ4"
   },
   "source": [
    "TensorFlow functions to observe\n",
    "\n",
    "\n",
    "*   tensordot - returns dot product\n",
    "*   cast - datatype casting on tensors\n",
    "*   reduce_mean - mean of TensorFlow datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LQhBBUb74U4M",
    "outputId": "8dfae224-4193-428e-bc5b-8ca1a4f4906a"
   },
   "outputs": [],
   "source": [
    "itrs_per_epoch = int(len(xtrain)/HP_batch_size)\n",
    "# Each iteration represents 1 epoch\n",
    "for i in range(HP_epochs):\n",
    "  per_iter_err = []\n",
    "  for j in range(itrs_per_epoch):\n",
    "    # get data and labels in fixed batch sizes\n",
    "    data, labels = itr.get_next()\n",
    "    \n",
    "    # simplest representation of ML \n",
    "    # y = mx + c\n",
    "    y = tf.tensordot(data, w, axes=1) + b\n",
    "    # decode the results, and typecast to double\n",
    "    y = myclassifier(y)\n",
    "    y = tf.cast(y, tf.dtypes.double)\n",
    "    # typecast the known labels to double\n",
    "    labels = tf.cast(labels, tf.dtypes.double)\n",
    "    # calculate the error\n",
    "    avg_error = tf.reduce_mean(tf.square(y - labels))\n",
    "    per_iter_err.append(avg_error)\n",
    "    mse_derivative = tf.reduce_mean(2 * ( y - labels ))\n",
    "\n",
    "    # update the weights and bias for the next epoch\n",
    "    weight_update = tf.reduce_mean(mse_derivative*data)\n",
    "    bias_update = tf.reduce_mean(mse_derivative)\n",
    "    \n",
    "    w = w - HP_lr * weight_update\n",
    "    b = b - HP_lr * bias_update\n",
    "    # end of iteration simulation\n",
    "\n",
    "  final_epoch_error = np.array(per_iter_err).mean()\n",
    "  err.append(final_epoch_error)\n",
    "  #end of epoch simulation\n",
    "  \n",
    "  print('****')\n",
    "  print('For epoch ',i+1,', the weights = ', w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3z3-RiPMmkCR"
   },
   "source": [
    "Phew! Observed how weights and bias correct themselves on each iteration, and each epoch? It's a long process!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-zIJ0Zlm0OX"
   },
   "source": [
    "Let's observe how the loss dropped over a period of time- on the y-axis we have the loss (mean squared error), and epochs on the x-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "MuQYSQW54b7F",
    "outputId": "8f9d1dbd-e3a8-41c0-e614-2c952427171a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "time_axis = range(1,len(err)+1)\n",
    "plt.plot(  time_axis, err) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6Hn101MnGjO"
   },
   "source": [
    "After this, we can locate the point on the graph with minimum y (because this is an error). That's the point at which we can then retrain the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJBLyFeznQ9U"
   },
   "source": [
    "**Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nv-MnjV_nW00"
   },
   "source": [
    "In this module we achieved the following:\n",
    "\n",
    "\n",
    "\n",
    "*   Revised the model training process\n",
    "*   Simulated the model training\n",
    "*   Observed inbuilt TensorFlow functions\n",
    "*   Initialized weights and bias\n",
    "*   Observed their updates over iterations and epochs\n",
    "*   Calculated and plotted the errors over epochs\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
